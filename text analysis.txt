Text Analytics 
1. Extract Sample document and apply following document preprocessing methods:  Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. 2. Create representation of document by calculating Term Frequency and Inverse Document  Frequency. 

sentence1 = "I will walk 500 miles and I would walk 500 more. Just to be the man who walks a thousand miles to fall down at your door!" 
sentence2 = "I played the play playfully as the players were playing in the play with playfulness" 

from nltk import word_tokenize 
print('Tokenized words:', word_tokenize(sentence1)) 

from nltk import pos_tag 
token = word_tokenize(sentence1) + word_tokenize(sentence2) 
tagged = pos_tag(token) 
print("Tagging Parts of Speech:", tagged) 

from nltk.corpus import stopwords 
stop_words = stopwords.words('english') 
token = word_tokenize(sentence1) 
cleaned_token = [] 
for word in token: 
    if word not in stop_words: 
        cleaned_token.append(word) 
print('Unclean version:', token) 
print('\nCleaned version:', cleaned_token) 

from nltk.stem import PorterStemmer 
stemmer = PorterStemmer() 
stemmed_tokens = [stemmer.stem(token) for token in cleaned_token] 
print(stemmed_tokens) 

from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer() 
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_token] 
print(lemmatized_tokens) 

from sklearn.feature_extraction.text import TfidfVectorizer 
tfidf_vectorizer = TfidfVectorizer() 
tfidf_matrix = tfidf_vectorizer.fit_transform([sentence1]) 
print(tfidf_matrix.toarray()) 
print("\n# FEATURED NAMES \n") 
print(tfidf_vectorizer.get_feature_names_out())

